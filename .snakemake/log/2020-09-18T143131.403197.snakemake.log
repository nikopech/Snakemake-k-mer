Building DAG of jobs...
Using shell: /usr/bin/bash
Provided cores: 4
Rules claiming more threads will be scaled down.
Job counts:
	count	jobs
	1	all
	1	best_fit
	3	k_mer_files
	5

[Fri Sep 18 14:31:31 2020]
Job 3: Call files/sample_dnaseq1.fasta with k=7

[Fri Sep 18 14:31:33 2020]
Finished job 3.
1 of 5 steps (20%) done

[Fri Sep 18 14:31:33 2020]
Job 1: Call files/sample_dnaseq1.fasta with k=5

[Fri Sep 18 14:31:35 2020]
Finished job 1.
2 of 5 steps (40%) done

[Fri Sep 18 14:31:35 2020]
Job 2: Call files/sample_dnaseq1.fasta with k=6

[Fri Sep 18 14:31:37 2020]
Finished job 2.
3 of 5 steps (60%) done

[Fri Sep 18 14:31:37 2020]
Job 4: Best kmer search

[Fri Sep 18 14:31:42 2020]
Error in rule best_fit:
    jobid: 4
    output: result.txt
    shell:
        
			python best-fit.py result.txt files/sample_dnaseq1.fasta
			
        (one of the commands exited with non-zero exit code; note that snakemake uses bash strict mode!)

Removing output files of failed job best_fit since they might be corrupted:
result.txt
Shutting down, this might take some time.
Exiting because a job execution failed. Look above for error message
Complete log: /home/togkousa/INEB work/snakemake/Snakemake-k-mer/.snakemake/log/2020-09-18T143131.403197.snakemake.log
